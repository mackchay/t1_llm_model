version: "3.9"

services:
  # === Ollama (LLM runtime) ===
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"   # Ollama API endpoint
    volumes:
      - ollama_models:/root/.ollama  # Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
    entrypoint: >
      sh -c "
      /bin/ollama serve &
      sleep 10 &&
      echo 'ðŸš€ ÐŸÐ¾Ð´Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ qwen2.5-coder:7b-instruct-q4_K_M...' &&
      ollama pull qwen2.5-coder:7b-instruct-q4_K_M &&
      tail -f /dev/null
      "

  # === FastAPI ÑÐµÑ€Ð²Ð¸Ñ ===
  api:
    build: .
    container_name: kpi-api
    restart: unless-stopped
    depends_on:
      - ollama
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    command: >
      sh -c "
      pip install -r requirements.txt &&
      uvicorn api:app --host 0.0.0.0 --port 8000 --reload
      "
    volumes:
      - .:/app
    working_dir: /app

volumes:
  ollama_models:
